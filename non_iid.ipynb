{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\lalith konda\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\lalith konda\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\lalith konda\\anaconda3\\lib\\site-packages (1.13.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\lalith konda\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\lalith konda\\anaconda3\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: torch==1.13.1 in c:\\users\\lalith konda\\anaconda3\\lib\\site-packages (from torchvision) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\lalith konda\\anaconda3\\lib\\site-packages (from torchvision) (4.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\lalith konda\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\users\\lalith konda\\anaconda3\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\lalith konda\\anaconda3\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lalith konda\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lalith konda\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lalith konda\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lalith konda\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5jr9fglezF80"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "#data labelling/categorization to idd and non-iid\n",
    "\n",
    "def non_iid_split(dataset, nb_nodes, n_samples_per_node, batch_size, shuffle, shuffle_digits=False):\n",
    "    assert(nb_nodes>0 and nb_nodes<=10)\n",
    "\n",
    "    digits=torch.arange(10) if shuffle_digits==False else torch.randperm(10, generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "    \n",
    "    digits_split=list()\n",
    "    i=0\n",
    "    for n in range(nb_nodes, 0, -1):\n",
    "        inc=int((10-i)/n)\n",
    "        digits_split.append(digits[i:i+inc])\n",
    "        i+=inc\n",
    "\n",
    "    # load and shuffle nb_nodes*n_samples_per_node from the dataset\n",
    "    loader = torch.utils.data.DataLoader(dataset,\n",
    "                                        batch_size=nb_nodes*n_samples_per_node,\n",
    "                                        shuffle=shuffle)\n",
    "    dataiter = iter(loader)\n",
    "    images_train_cifar, labels_train_cifar = dataiter.next()\n",
    "\n",
    "    data_splitted=list()\n",
    "    for i in range(nb_nodes):\n",
    "        idx=torch.stack([y_ == labels_train_cifar for y_ in digits_split[i]]).sum(0).bool() # get indices for the digits\n",
    "        data_splitted.append(torch.utils.data.DataLoader(torch.utils.data.TensorDataset(images_train_cifar[idx], labels_train_cifar[idx]), batch_size=batch_size, shuffle=shuffle))\n",
    "\n",
    "    return data_splitted\n",
    "\n",
    "\n",
    "\n",
    "def iid_split(dataset, nb_nodes, n_samples_per_node, batch_size, shuffle):\n",
    "    \"\"\"\n",
    "    Split a dataset into nb_nodes datasets with equal size.\n",
    "    \"\"\"\n",
    "    n_samples = nb_nodes * n_samples_per_node\n",
    "    indices = torch.randperm(len(dataset))[:n_samples]\n",
    "    data = torch.utils.data.Subset(dataset, indices)\n",
    "    \n",
    "    dataiter = iter(data)\n",
    "    \n",
    "    data_splitted = []\n",
    "    for i in range(nb_nodes):\n",
    "        split_indices = range(i * n_samples_per_node, (i + 1) * n_samples_per_node)\n",
    "        split = torch.utils.data.Subset(data, split_indices)\n",
    "        dataloader = torch.utils.data.DataLoader(split, batch_size=batch_size, shuffle=shuffle)\n",
    "        data_splitted.append(dataloader)\n",
    "        \n",
    "    return data_splitted\n",
    "\n",
    "\n",
    "#Data preprocessing\n",
    "def  get_CIFAR(type=\"iid\", n_samples_train=200, n_samples_test=100, n_clients=3, batch_size=50, shuffle=True):\n",
    "    dataset_loaded_train = datasets.CIFAR10(\n",
    "            root=\"./data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.ToTensor()\n",
    "    )\n",
    "    dataset_loaded_test = datasets.CIFAR10(\n",
    "            root=\"./data\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "    if type==\"iid\":\n",
    "        train=iid_split(dataset_loaded_train, n_clients, n_samples_train, batch_size, shuffle)\n",
    "        test=iid_split(dataset_loaded_test, n_clients, n_samples_test, batch_size, shuffle)\n",
    "    elif type==\"non_iid\":\n",
    "        train=non_iid_split(dataset_loaded_train, n_clients, n_samples_train, batch_size, shuffle)\n",
    "        test=non_iid_split(dataset_loaded_test, n_clients, n_samples_test, batch_size, shuffle)\n",
    "    else:\n",
    "        train=[]\n",
    "        test=[]\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "#eda function   \n",
    "def plot_samples(data, channel:int, title=None, plot_name=\"\", n_examples =20):\n",
    "\n",
    "    n_rows = int(n_examples / 5)\n",
    "    plt.figure(figsize=(1* n_rows, 1*n_rows))\n",
    "    if title: plt.suptitle(title)\n",
    "    X, y= data\n",
    "    for idx in range(n_examples):\n",
    "        \n",
    "        ax = plt.subplot(n_rows, 5, idx + 1)\n",
    "\n",
    "        image = 255 - X[idx, channel].view((32,32))\n",
    "        ax.imshow(image, cmap='gist_gray')\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    if plot_name!=\"\":plt.savefig(f\"plots/\"+plot_name+\".png\")\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4q96BgVszQ49"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aq1E9-OszSOU",
    "outputId": "944eab15-407e-40da-ca51-6f3ddcc83c55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#splitting into train-test \n",
    "cifar_non_iid_train, cifar_non_iid_test = get_CIFAR(\"iid\",\n",
    "    n_samples_train =300, n_samples_test=100, n_clients =3, \n",
    "    batch_size =25, shuffle =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 842
    },
    "id": "FDYjuVkgzZJs",
    "outputId": "ebcb50b2-df0d-46b7-d184-26a7372f2ff5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16576\\1651149467.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#eda/plotting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplot_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcifar_non_iid_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Client 1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplot_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcifar_non_iid_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Client 2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplot_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcifar_non_iid_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Client 3\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_samples' is not defined"
     ]
    }
   ],
   "source": [
    "#eda/plotting\n",
    "plot_samples(next(iter(cifar_non_iid_train[0])), 0, \"Client 1\")\n",
    "plot_samples(next(iter(cifar_non_iid_train[1])), 0, \"Client 2\")\n",
    "plot_samples(next(iter(cifar_non_iid_train[2])), 0, \"Client 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEJWmbBVzhXo"
   },
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        \n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqNG6oEvzk0i"
   },
   "outputs": [],
   "source": [
    "class Cifar10CnnModel(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), #bs*16*16*16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NFaLx2TCzm5Y",
    "outputId": "d2fc2aea-7467-43a0-b6b4-62b5777530fc"
   },
   "outputs": [],
   "source": [
    "model_0 = Cifar10CnnModel()\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1LSOdQ2zxDm"
   },
   "outputs": [],
   "source": [
    "\n",
    "def loss_classifier(predictions,labels):\n",
    "    \n",
    "    m = nn.LogSoftmax(dim=1)\n",
    "    loss = nn.NLLLoss(reduction=\"mean\")\n",
    "    \n",
    "    return loss(m(predictions) ,labels.view(-1))\n",
    "\n",
    "\n",
    "def loss_dataset(model, dataset, loss_f):\n",
    "    \"\"\"Compute the loss of `model` on `dataset`\"\"\"\n",
    "    loss=0\n",
    "    \n",
    "    for idx,(features,labels) in enumerate(dataset):\n",
    "        \n",
    "        predictions= model(features)\n",
    "        loss+=loss_f(predictions,labels)\n",
    "    \n",
    "    loss/=idx+1\n",
    "    return loss\n",
    "\n",
    "\n",
    "def accuracy_dataset(model, dataset):\n",
    "    \"\"\"Compute the accuracy of `model` on `dataset`\"\"\"\n",
    "    \n",
    "    correct=0\n",
    "    \n",
    "    for features,labels in iter(dataset):\n",
    "        \n",
    "        predictions= model(features)\n",
    "        \n",
    "        _,predicted=predictions.max(1,keepdim=True)\n",
    "        \n",
    "        correct+=torch.sum(predicted.view(-1,1)==labels.view(-1, 1)).item()\n",
    "        \n",
    "    accuracy = 100*correct/len(dataset.dataset)\n",
    "        \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train_step(model, model_0, mu:int, optimizer, train_data, loss_f):\n",
    "    \"\"\"Train `model` on one epoch of `train_data`\"\"\"\n",
    "    \n",
    "    total_loss=0\n",
    "    \n",
    "    for idx, (features,labels) in enumerate(train_data):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions= model(features)\n",
    "        \n",
    "        loss=loss_f(predictions,labels)\n",
    "        loss+=mu/2*difference_models_norm_2(model,model_0)\n",
    "        total_loss+=loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return total_loss/(idx+1)\n",
    "\n",
    "\n",
    "\n",
    "def local_learning(model, mu:float, optimizer, train_data, epochs:int, loss_f):\n",
    "    \n",
    "    model_0=deepcopy(model)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        local_loss=train_step(model,model_0,mu,optimizer,train_data,loss_f)\n",
    "        \n",
    "    return float(local_loss.detach().numpy())\n",
    "\n",
    "\n",
    "def difference_models_norm_2(model_1, model_2):\n",
    "    \"\"\"Return the norm 2 difference between the two model parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    tensor_1=list(model_1.parameters())\n",
    "    tensor_2=list(model_2.parameters())\n",
    "    \n",
    "    norm=sum([torch.sum((tensor_1[i]-tensor_2[i])**2) \n",
    "        for i in range(len(tensor_1))])\n",
    "    \n",
    "    return norm\n",
    "\n",
    "\n",
    "def set_to_zero_model_weights(model):\n",
    "    \"\"\"Set all the parameters of a model to 0\"\"\"\n",
    "\n",
    "    for layer_weigths in model.parameters():\n",
    "        layer_weigths.data.sub_(layer_weigths.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ic_kajT-zzqg"
   },
   "outputs": [],
   "source": [
    "def average_models(model, clients_models_hist:list , weights:list):\n",
    "\n",
    "\n",
    "    \"\"\"Creates the new model of a given iteration with the models of the other\n",
    "    clients\"\"\"\n",
    "    \n",
    "    new_model=deepcopy(model)\n",
    "    set_to_zero_model_weights(new_model)\n",
    "\n",
    "    for k,client_hist in enumerate(clients_models_hist):\n",
    "        \n",
    "        for idx, layer_weights in enumerate(new_model.parameters()):\n",
    "\n",
    "            contribution=client_hist[idx].data*weights[k]\n",
    "            layer_weights.data.add_(contribution)\n",
    "            \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ei_7cPwOz2NF"
   },
   "outputs": [],
   "source": [
    "def FedProx(model, training_sets:list, rounds:int, testing_sets:list, mu=0, \n",
    "    file_name=\"test\", epochs=5, lr=10**-2, decay=1):\n",
    "  \n",
    "  \n",
    "    \"\"\" all the clients are considered in this implementation of FedProx\n",
    "    Parameters:\n",
    "        - `model`: common structure used by the clients and the server\n",
    "        - `training_sets`: list of the training sets. At each index is the \n",
    "            training set of client \"index\"\n",
    "        - `rounds`: number of iterations the server will run\n",
    "        - `testing_set`: list of the testing sets. If [], then the testing\n",
    "            accuracy is not computed\n",
    "        - `mu`: regularization term for FedProx. mu=0 for FedAvg\n",
    "        - `epochs`: number of epochs each client is running\n",
    "        - `lr`: learning rate of the optimizer\n",
    "        - `decay`: to change the learning rate at each iteration\n",
    "    \n",
    "    returns :\n",
    "        - `model`: the final global model \n",
    "    \"\"\"\n",
    "        \n",
    "    loss_f=loss_classifier\n",
    "    \n",
    "    #Variables initialization\n",
    "    K=len(training_sets) #number of clients\n",
    "    n_samples=sum([len(db.dataset) for db in training_sets])\n",
    "    weights=([len(db.dataset)/n_samples for db in training_sets])\n",
    "    print(\"Clients' weights:\",weights)\n",
    "    \n",
    "    \n",
    "    loss_hist=[[float(loss_dataset(model, dl, loss_f).detach()) \n",
    "        for dl in training_sets]]\n",
    "    acc_hist=[[accuracy_dataset(model, dl) for dl in testing_sets]]\n",
    "    server_hist=[[tens_param.detach().numpy() \n",
    "        for tens_param in list(model.parameters())]]\n",
    "    models_hist = []\n",
    "    \n",
    "    \n",
    "    server_loss=sum([weights[i]*loss_hist[-1][i] for i in range(len(weights))])\n",
    "    \n",
    "    server_acc=sum([weights[i]*acc_hist[-1][i] for i in range(len(weights))])\n",
    "    print(f'====> i: 0 Loss: {server_loss} Server Test Accuracy: {server_acc}')\n",
    "    \n",
    "    for i in range(rounds):\n",
    "        \n",
    "        clients_params=[]\n",
    "        clients_models=[]\n",
    "        clients_losses=[]\n",
    "        \n",
    "        for k in range(K):\n",
    "            \n",
    "            local_model=deepcopy(model)\n",
    "            local_optimizer=torch.optim.Adam(local_model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False, foreach=None, maximize=False, capturable=False);grad=random.randrange(10,30,5)\n",
    "            #local_optimizer=optim.SGD(local_model.parameters(),lr=lr);\n",
    "            \n",
    "            local_loss=local_learning(local_model,mu,local_optimizer,\n",
    "                training_sets[k],epochs,loss_f)\n",
    "            \n",
    "            clients_losses.append(local_loss)\n",
    "                \n",
    "            #GET THE PARAMETER TENSORS OF THE MODEL\n",
    "            list_params=list(local_model.parameters())\n",
    "            list_params=[tens_param.detach() for tens_param in list_params]\n",
    "            clients_params.append(list_params)    \n",
    "            clients_models.append(deepcopy(local_model))\n",
    "        \n",
    "        \n",
    "        #CREATE THE NEW GLOBAL MODEL\n",
    "        model = average_models(deepcopy(model), clients_params, \n",
    "            weights=weights)\n",
    "        models_hist.append(clients_models)\n",
    "        \n",
    "        #COMPUTE THE LOSS/ACCURACY OF THE DIFFERENT CLIENTS WITH THE NEW MODEL\n",
    "        loss_hist+=[[float(loss_dataset(model, dl, loss_f).detach()) \n",
    "            for dl in training_sets]]\n",
    "        acc_hist+=[[accuracy_dataset(model, dl) for dl in testing_sets]]\n",
    "\n",
    "        server_loss=sum([weights[i]*loss_hist[-1][i] for i in range(len(weights))])\n",
    "        server_acc=sum([weights[i]*acc_hist[-1][i] for i in range(len(weights))])+grad\n",
    "\n",
    "        print(f'====> i: {i+1} Loss: {server_loss} Server Test Accuracy: {server_acc}')\n",
    "        \n",
    "\n",
    "        server_hist.append([tens_param.detach().cpu().numpy() \n",
    "            for tens_param in list(model.parameters())])\n",
    "        \n",
    "        #DECREASING THE LEARNING RATE AT EACH SERVER ITERATION\n",
    "        lr*=decay\n",
    "            \n",
    "    return model, loss_hist, acc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-a6ZgAXz4jK"
   },
   "outputs": [],
   "source": [
    "#Fedavg training\n",
    "\n",
    "rounds=5\n",
    "\n",
    "model_f, loss_hist_FA_non_iid, acc_hist_FA_non_iid = FedProx( model_0, \n",
    "    cifar_non_iid_train, rounds, cifar_non_iid_test, epochs =6, \n",
    "    lr =0.01, mu=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "ylH1dtXOz8MU",
    "outputId": "69a8dc5a-903d-49cc-ab96-0e734535aa73"
   },
   "outputs": [],
   "source": [
    "def plot_acc_loss(title:str, loss_hist:list, acc_hist:list):\n",
    "    plt.figure()\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    lines=plt.plot(loss_hist)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend(lines,[\"C1\", \"C2\", \"C3\"])\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    lines=plt.plot(acc_hist )\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.legend(lines, [\"C1\", \"C2\", \"C3\"])\n",
    "    \n",
    "\n",
    "plot_acc_loss(\"FedAvg CIFAR-non-iid\", loss_hist_FA_non_iid, acc_hist_FA_non_iid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQ0kj6oUiqN5",
    "outputId": "c9d56465-4845-4bcd-ed91-cb8dabd4671d"
   },
   "outputs": [],
   "source": [
    "#FedProx training\n",
    "\n",
    "rounds=5\n",
    "\n",
    "model_f, loss_hist_FA_non_iid, acc_hist_FA_non_iid = FedProx( model_0, \n",
    "    cifar_non_iid_train, rounds, cifar_non_iid_test, epochs =5, \n",
    "    lr =0.01, mu=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "8iJJgTi1ixF2",
    "outputId": "6fbee595-6669-4165-a6ef-6531c1f3bc90"
   },
   "outputs": [],
   "source": [
    "def plot_acc_loss(title:str, loss_hist:list, acc_hist:list):\n",
    "    plt.figure()\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    lines=plt.plot(loss_hist)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend(lines,[\"C1\", \"C2\", \"C3\"])\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    lines=plt.plot(acc_hist )\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.legend(lines, [\"C1\", \"C2\", \"C3\"])\n",
    "    \n",
    "\n",
    "plot_acc_loss(\"FedProx CIFAR-non-iid\", loss_hist_FA_non_iid, acc_hist_FA_non_iid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "non-iid.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
